{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaled Dot-Product Attention: \n",
    "\n",
    "The 2017 famous paper, by Google, [Attention Is All You Need](https://arxiv.org/abs/1706.03762) introduced the Transformer model and scaled dot-product attention, sometimes also called QKV (**Q**ueries, **K**eys, **V**alues) attention. Since then, Transformers have come to dominate large-scale natural language applications. Scaled dot-product attention can be used to improve seq2seq models too. \n",
    "\n",
    "In this notebook, we'll implement a simplified version of scaled dot-product attention and replicate word alignment between English and French, as shown in [Bhadanau, et al. (2014)](https://arxiv.org/abs/1409.0473).\n",
    "\n",
    "The Transformer model learns how to align words in different languages. We won't be training any weights here, but instead we'll be using some [pre-trained aligned word embeddings from here](https://fasttext.cc/docs/en/aligned-vectors.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02701261",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install numpy matplotlib pickle5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from utils import softmax, tokenize, embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a3265ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/word2int_en.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\github\\NLP-With-Attention-Models\\1 neural machine translation\\2_scaled_dot_product_QKV_attention.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/github/NLP-With-Attention-Models/1%20neural%20machine%20translation/2_scaled_dot_product_QKV_attention.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Load the word2int dictionaries\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/github/NLP-With-Attention-Models/1%20neural%20machine%20translation/2_scaled_dot_product_QKV_attention.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39m./data/word2int_en.pkl\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/github/NLP-With-Attention-Models/1%20neural%20machine%20translation/2_scaled_dot_product_QKV_attention.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     en_words \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(f)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/github/NLP-With-Attention-Models/1%20neural%20machine%20translation/2_scaled_dot_product_QKV_attention.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m./data/word2int_fr.pkl\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[1;32mc:\\Users\\skand\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/word2int_en.pkl'"
     ]
    }
   ],
   "source": [
    "# Load the word2int dictionaries\n",
    "with open(\"./data/word2int_en.pkl\", \"rb\") as f:\n",
    "    en_words = pickle.load(f)\n",
    "    \n",
    "with open(\"./data/word2int_fr.pkl\", \"rb\") as f:\n",
    "    fr_words = pickle.load(f)\n",
    "\n",
    "# Load the word embeddings\n",
    "en_embeddings = np.load(\"./data/embeddings_en.npz\")[\"embeddings\"]\n",
    "fr_embeddings = np.load(\"./data/embeddings_fr.npz\")[\"embeddings\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176fd2aa",
   "metadata": {},
   "source": [
    "The scaled-dot product attention consists of two matrix multiplications and a softmax scaling as shown in the diagram below from [Vaswani, et al. (2017)](https://arxiv.org/abs/1706.03762). It takes three input matrices, the queries, keys, and values.\n",
    "\n",
    "![scaled-dot product attention diagram](./images/attention.png)\n",
    "\n",
    "Mathematically, this is expressed as\n",
    "\n",
    "$$ \n",
    "\\large \\mathrm{Attention}\\left(Q, K, V\\right) = \\mathrm{softmax}\\left(\\frac{QK^{\\top}}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "where :\n",
    "- $Q$, $K$, and $V$ are the queries, keys, and values matrices respectively, \n",
    "- and $d_k$ is the dimension of the keys. \n",
    "\n",
    "In practice, Q, K, and V all have the same dimensions. This form of attention is faster and more space-efficient than the basic attention since it consists of only matrix multiplications instead of a learned feed-forward layer.\n",
    "\n",
    "Conceptually, the first matrix multiplication is a measure of the similarity between the queries and the keys. This is transformed into weights using the softmax function. These weights are then applied to the values with the second matrix multiplication resulting in output attention vectors. Typically, decoder states are used as the queries while encoder states are the keys and values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10b0826",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "Implement the softmax function with Numpy and use it to calculate the weights from the queries and keys. Assume the queries and keys are 2D arrays (matrices). Note that since the dot-product of Q and K will be a matrix, you'll need to take care to calculate softmax over a specific axis. See the end of the notebook for solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae782d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_weights(queries, keys):\n",
    "    \"\"\" Calculate the weights for scaled dot-product attention\"\"\"\n",
    "\n",
    "    dot = np.matmul(queries, keys.T)/np.sqrt(keys.shape[1])\n",
    "    weights = softmax(dot, axis=1)\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86727e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementation of visualize_alignment, tokenize and embed is found in utils.py\n",
    "\n",
    "\n",
    "# Tokenize example sentences in English and French, then get their embeddings\n",
    "sentence_en = \"The agreement on the European Economic Area was signed in August 1992 .\"\n",
    "\n",
    "tokenized_en = tokenize(sentence_en, en_words)\n",
    "embedded_en = embed(tokenized_en, en_embeddings)\n",
    "\n",
    "sentence_fr = \"L accord sur la zone économique européenne a été signé en août 1992 .\"\n",
    "tokenized_fr = tokenize(sentence_fr, fr_words)\n",
    "embedded_fr = embed(tokenized_fr, fr_embeddings)\n",
    "\n",
    "# These weights indicate alignment between words in English and French\n",
    "alignment = calculate_weights(embedded_fr, embedded_en)\n",
    "\n",
    "# Visualize weights to check for alignment\n",
    "visualize_alignment(alignment, sentence_en, sentence_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f969b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_qkv(queries, keys, values):\n",
    "    \"\"\" Calculate scaled dot-product attention from queries, keys, and values matrices \"\"\"\n",
    "    \n",
    "    weights = calculate_weights(queries, keys)\n",
    "    return np.matmul(weights, values)\n",
    "    pass\n",
    "\n",
    "\n",
    "attention_qkv_result = attention_qkv(embedded_fr, embedded_en, embedded_en)\n",
    "\n",
    "print(f\"The shape of the attention_qkv function is {attention_qkv_result.shape}\")\n",
    "print(f\"Some elements of the attention_qkv function are \\n{attention_qkv_result[0:2,:10]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
